项目编号	20171027
文档编号	01
密    级	内部













CanToolApp测试报告





V1.0













                                 天津大学计算机学院







评    审   日  期： 2017年11月5日
 
目 录
1导言	3
1.1目的	3
1.2范围	3
1.3缩写说明	3
1.4术语定义	3
1.5引用标准	4
1.6参考资料	4
1.7版本更新信息	4
2. 测试时间、地点和人员	5
3 测试环境描述	5
4测试执行情况	6
4.1 单元测试执行情况	6
４.2功能测试执行情况	7
4.2.1	7
4.2.2	7
4.2.3	8
4.2.4	8
4.2.5	9
5测试结果分析	10
5.1 测试进度和工作量度量	10
5.1.1 进度度量	11
5.1.2 工作量度量	11
5.2 缺陷数据度量	11
5.3 综合数据分析	12
6 测试评估	13
6.1 测试任务评估	13
6.2 测试对象评估	13
 
 
1导言
1.1目的
该文档的目的是描述CanToolApp上位机Windows客户端测试的总结报告，其主要内容包括：
	系统环境简介
	系统数据度量
	系统结果评估

本文档的预期读者是：
	项目管理人员
	测试人员

1.2范围
该文档定义了CanToolApp上位机Windows客户端测试和CanTool装置测试的结果，给出了各个模块的单元测试，总结了测试客户端的数据显示、数据解析、数据存储、数据发送的基本功能，以及实时响应的性能，给出了测试的结论。

1.3缩写说明

1.4术语定义

LoadRunner
Mercury Interactive的一个对Windows和UniX环境的负载测试工具。
单元测试
    是指对软件中的最小可测试单元进行检查和验证。
功能性测试
按照系统需求定义中的功能定义部分对系统实行的系统级别的测试。
性能测试
    性能测试是通过自动化的测试工具模拟多种正常、峰值以及异常负载条件来对系统的各项性能指标进行测试。
测试用例
测试人员设计出来的用来测试软件某个功能的一种情形。  


1.5引用标准
[1] 
[2] 

1.6参考资料
 [1] 《软件测试：第二版》 
	Ron Patton 著/机械工业出版社

1.7版本更新信息
本文档的更新信息如表G－１．

表G-1 版本更新记录
修改编号	修改日期	修改后版本	修改位置	修改内容概述
000	2017.10.10	0.1	全部	初始发布版本
001	2017.10.27	1.0	第4章	修改
				


2. 测试时间、地点和人员
本次测试的时间、地点和人员总结如下：
	测试时间：2017-10-10至2017-11-4，基本按照计划进行。
	地点：天津大学计算机学院
	人员：测试组的全体成员共计4人

3 测试环境描述
本测试的测试目的是验证各个模块是否工作正常，验证客户端的数据显示、数据解析、数据存储、数据发送的基本功能，以及实时响应的性能。

这个测试机器的配置环境如下：
	操作系统：Micrsoft windowXP Professional SP1
	浏览器：Micrsoft IE 6.0.2800.1106
	CPU：P4 2.8G
	内存：512M
	硬盘：80G

4测试执行情况
测试数据总结如下。
4.1单元测试执行情况
模块功能	本模块实现的主要功能
现有资源	模块1	模块1包含的文件
	模块2	模块2包含的文件
	… … … …
	模块n	模块n包含的文件
程序结构	功能点	涉及的方法
	功能名称1	方法1：public StringBuffer DrawExitContent(int nLangType, String strComm,String strUComm)
功能说明：列出已经存在的内容检索配置
参数描述：	int nLangType ： 语种信息
	String strComm： “正常”字符串
String strUComm  “删除”字符串
	输出：以表格的形式列出已经存在的内容检索
	异常：输出空的字符串
	流程：	组合查询条件
读取数据库内容
while 循环结束?
读取数据库记录
            关闭数据集 
            返回结果
方法2：
     功能说明：
     参数描述：
     输出：
     异常：
     流程：
… …    … …     … …   … …
方法n：
     功能说明：
     参数描述：
     输出：
     异常：
     流程：
	… …   … …   … …   … …   … …   … …   … …
	功能名称n	方法1：
     功能说明：
     参数描述：
     输出：
     异常：
     流程：
… …    … …     … …   … …
方法n：
     功能说明：
     参数描述：
     输出：
     异常：
     流程：
测试用例	测试功能点	用例描述	用例测试结果
	功能点1	方法一：使用测试程序实现单元测试，例如：
public void testCheckExitIndex() {
        boolean  expected = contentSetup.CheckExitIndex(1);
        boolean bActual = true;
        assertEquals(expected, bActual);
} 
方法二：使用界面进行单元测试，将界面上的操作使用文字描述清楚，例如：
输入要查询的用户名，依次选择【用户】、【名称】，选中【使用模糊查询】，点击【确定】按钮后，浏览界面能正确的显示所有满足名称模糊查询条件的用户；

这两种方法可以选择适合自己程序的一种来进行单元测试。	
	… …   … …   … …   … …   … …   … …   … …
	功能点n	方法一：使用测试程序实现单元测试；
方法二：使用界面进行单元测试，将界面上的操作使用文字描述清楚。 

这两种方法可以选择适合自己程序的一种来进行单元测试。	
测试结果	单元测试是否通过？

4.2 功能测试执行情况

表G-2是测试用例执行情况的综述。

表G-2:测试用例的度量数据
被测对象
	用例
	执行总数
	发现缺陷数

Arduino接收数据页面	TestCase-FUNC-01
TestCase-FUNC-02
TestCase-FUNC-03
TestCase-FUNC-04	8＋1	3
Arduino发送数据页面	TestCase-FUNC-05
TestCase-FUNC-06	29＋1	9
上位机主页	TestCase-FUNC-05
TestCase-FUNC-07	12＋1	1
上位机数据接收页面	TestCase-FUNC-05
TestCase-FUNC-08	10＋1	1
上位机数据解析页面	TestCase-FUNC-05
TestCase-FUNC-09	4＋1	2
上位机数据显示页面	TestCase-FUNC-05
TestCase-FUNC-10	6＋1	6
用户自定义页面——本机可用COM口搜索、波特率	TestCase-FUNC-05
TestCase-FUNC-11	1＋1	1
用户自定义页面——CAN速率设置、Open、Close	TestCase-FUNC-05
TestCase-FUNC-12	0＋1	0
CAN信息显示页面——可选是否仪表盘方式显示			
物理图像界面——用户自主选择接收到CAN信号的曲线	TestCase-FUNC-05
TestCase-FUNC-13	1＋2	0
上位机GUI发送界面	TestCase-Perf-1	8＋1	8
实时保存为数据文件——CAN信息数据、用户自定义数据、CAN信号物理值显示方式	TestCase-Perf-2	0＋1	0
CAN信号与CAN信息相互转化准确度			
信号数据库数据提取和存储功能实现			

4.1.1 Arduino信息接收功能

4.1.2 Arduino 信息发送功能
4.1.3 WindowsApp客户端信息接收功能
4.1.4 WindowsApp客户端信息发送功能
4.1.5 WindowsApp客户端信息解析功能
4.1.6 WindowsApp客户端信息存储功能


5测试结果分析

从测试的时间、工作量、缺陷等角度分析如下。

5.1 测试进度和工作量度量 
功能测试的进度和工作量计划与实际比较结果如下。

5.1.1 进度度量

表G－4是测试进度的计划与实际结果比较。从度量数据看实际进度与计划基本相符。

表G－4：是测试进度的度量数据
任务	计划开始	计划结束	实际开始	实际结束
测试计划与设计	2006-5-19	2006-5-26	2006-5-15	2006-6-1
测试执行	2006-6-12	2006-7-2	2006-6-17	2006-7-1
测试总结	2006-7-3	2006-7-5	2006-6-30	2006-7-1

5.1.2 工作量度量 
表G－5是实际测试工作量的数据，与计划基本相符。

表G－5：是测试工作量度量
执行任务	开始时间	结束时间	工作量
（人时）
测试计划与设计	2006-5-15	2006-6-1	20×3人时
测试执行	2006-6-17	2006-7-1	15×3人时
测试总结	2006-6-30	2006-7-1	6×3人时

5.2 缺陷数据度量 
表G－6是测试过程缺陷数据的总结情况，从缺陷的严重程度看，严重和致命的缺陷占的比例较大，说明系统还是存在严重问题，需要重新修改，产品不能提交。从缺陷的类型看，功能缺陷、赋值缺陷和设计缺陷占比较大的比例，说明设计和编码过程中存在很大的问题。

表G－6：测试过程的缺陷数据
被测对象	总数	严重程度	缺陷类型
		致命	严重	一般	提示	设计错误	赋值错误	算法错误	接口错误	功能错误	其它
首页	3	0	1	0	2	2	1	0	0	0	0
填写基本信息页面	9	0	0	4	5	0	0	0	0	9	0
填写工作经历页面	1	1	0	0	0	0	1	0	0	0	0
填写教育经历页面	1	1	0	0	0	0	1	0	0	0	0
填写工作技能页面	2	2	0	0	0	0	2	0	0	0	0
填写家庭关系页面	6	6	0	0	0	0	6	0	0	0	0
填写自我介绍页面	1	1	0	0	0	0	1	0	0	0	0
回答问卷界面	0	0	0	0	0	0	0	0	0	0	0
直接访问后续页面	8	0	8	0	0	0	8	0	0	0	0

5.3 综合数据分析 
根据测试的结果可以得出如下的分析数据。
用例执行效率 
＝ 执行用例总数 / 执行总时间（小时）
＝ 128 / 45 
＝ 2.84（个/小时）

用例质量
＝ 缺陷总数 / 用例总数 × 100
＝ 31 / 128 *100
＝ 24.2％


根据缺陷跟踪结果得出缺陷的严重程序分布和的缺陷的类型分布，如图G－8和图G－9。

缺陷严重程度分布饼图： 
 





缺陷类型分布饼图： 
 






6 测试评估

6.1 测试任务评估
	本次测试执行准备充足，完成了既定目标。但由于经验以及对工具使用不熟练，因此对系统性能测试还有待提高和加强。

6.2 测试对象评估
	测试对象不符合测试阶段质量要求，存在较多的缺陷，尤其是缓冲区溢出缺陷，所以，不能进入下一个阶段，这个客户端的系统需要进行修正，而后重新组织系统测试。
